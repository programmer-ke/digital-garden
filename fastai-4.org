#+FILETAGS: :fastai:machine_learning:deep_learning:nlp:

* Self-supervised learning              :self_supervised:nlp:language_models:

Self-supervised learning occurs when the label exist within the
training data itself. For example in NLP, teaching the model how to
predict the next word by feeding it a large corpus of text.

This creates what is called a language model, i.e. a model that has
been trained to guess what the next word of a text is.

We typically fine tune pre-trained language models to teach them the
vocabulary of the task at hand.

Self-supervised models are sometimes used on their own
e.g. for auto-complete, but are often used as pre-trained models fine
tuned for some other task e.g. classification, chat

* State of the art text classifier                   :text_classifier:ulmfit:

Steps taken to create a state of the art text classifier are:

 1) Train a sequential model with a large corpus of the language
    e.g. whole of wikipedia
 2) Fine tune the language model with sample text of the topic to be
    classified e.g. amazon reviews
 3) Fine tune into a classification model using labels on the training
    data e.g. labelled amazon reviews (positive or negative)

* Data prep for language model with fastai  :language_model:data_preparation:

Three steps needed are:
 1) Tokenization: The language is broken down into tokens representing
    words, numbers and special characters.
 2) Numericalization: The tokens are assembled into the vocabulary of the
    model and are replaced by their indices in the vocabulary
 3) Language Model DataLoader specification: Among other things,
    handles offsetting of the dependent and independent variables.

* Tokenization                                                 :tokenization:

Tokenization is breaking down a large chunk of text into small
words/subwords that can be used to make predictions of the next word.

Three approaches:
 - word-based tokenization :: the text is broken down into individual
   words
 - subword-based tokenization :: the text is broken down into
   collections of characters making up words
 - character-based tokenization :: the text is broken down into
   individual characters

** Fastai tokenization                                         :fastai:xxbos:
Some rules applied by fastai during tokenization are:

 - Replace repeated (non-significant) whitespace with a single space.
 - Insert a special character xxbos representing the beginning of the
   text (beginning of stream)
 - Insert special characters signifying upper case and capitalized
   words then lowercasing all words.
 - Insert special characters signfying repeated words and
   characters. This aids compression of the input saving space.

* Numericalization                                   :numericalization:vocab:

Numericalization is the process of converting the tokens into numbers
according to the vocabulary.

The vocabulary can either be infered (e.g. constructed based on the
most common tokens) or provided explicitly.

The vocabulary is represented as an ordered list of tokens and each
token in the stream is replaced by its index in this list.

** xxunk                                                              :xxunk:

Words who's frequency is below a certain threshold, for instance,
outside the most common x words or who's frequency is below the
minimum configured can be assigned the 'unknown word' token e.g. xxunk
in fastai

This reduces space required by the model and typically there's not
enough relevant data to train the model about these words.

* LMDataLoader                                                 :LMDataLoader:

Data loading process for each epoch.

1) shuffle all the entries (separate documents)
2) concatenate into single stream e.g. 50k tokens
3) Divide into mini streams according block size. e.g a block size of
   5 will give 5 mini streams of length 1000 in order: 0 - 999, 1000 -
   1999 etc
4) For each mini-stream, while retaining their order, select an offset
   of a certain sequence length and consecutively feed batches of the
   given size (block size * sequence length) in order to the model.

What is "self-supervised learning"?
What is a "language model"?
Why is a language model considered self-supervised?
What are self-supervised models usually used for?
Why do we fine-tune language models?
What are the three steps to create a state-of-the-art text classifier?
How do the 50,000 unlabeled movie reviews help us create a better text classifier for the IMDb dataset?
What are the three steps to prepare your data for a language model?
What is "tokenization"? Why do we need it?
Name three different approaches to tokenization.
What is xxbos?
List four rules that fastai applies to text during tokenization.
Why are repeated characters replaced with a token showing the number of repetitions and the character that's repeated?
What is "numericalization"?
Why might there be words that are replaced with the "unknown word" token?
With a batch size of 64, the first row of the tensor representing the
first batch contains the first 64 tokens for the dataset. What does
the second row of that tensor contain? What does the first row of the
second batch contain? (Carefulâ€”students often get this one wrong! Be
sure to check your answer on the book's website.)

Why do we need padding for text classification? Why don't we need it for language modeling?
What does an embedding matrix for NLP contain? What is its shape?
What is "perplexity"?
Why do we have to pass the vocabulary of the language model to the classifier data block?
What is "gradual unfreezing"?
Why is text generation always likely to be ahead of automatic identification of machine-generated texts?
