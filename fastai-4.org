#+FILETAGS: :fastai:machine_learning:deep_learning:nlp:

* Self-supervised learning              :self_supervised:nlp:language_models:

Self-supervised learning occurs when the label exist within the
training data itself. For example in NLP, teaching the model how to
predict the next word by feeding it a large corpus of text.

This creates what is called a language model, i.e. a model that has
been trained to guess what the next word of a text is.

We typically fine tune pre-trained language models to teach them the
vocabulary of the task at hand.

Self-supervised models are sometimes used on their own
e.g. for auto-complete, but are often used as pre-trained models fine
tuned for some other task e.g. classification, chat

* State of the art text classifier                   :text_classifier:ulmfit:

Steps taken to create a state of the art text classifier are:

 1) Train a sequential model with a large corpus of the language
    e.g. whole of wikipedia
 2) Fine tune the language model with sample text of the topic to be
    classified e.g. amazon reviews
 3) Fine tune into a classification model using labels on the training
    data e.g. labelled amazon reviews (positive or negative)

* Data prep for language model with fastai  :language_model:data_preparation:

Three steps needed are:
 1) Tokenization: The language is broken down into tokens representing
    words, numbers and special characters.
 2) Numericalization: The tokens are assembled into the vocabulary of the
    model and are replaced by their indices in the vocabulary
 3) Language Model DataLoader specification: Among other things,
    handles offsetting of the dependent and independent variables.

* Tokenization                                                 :tokenization:

Tokenization is breaking down a large chunk of text into small
words/subwords that can be used to make predictions of the next word.

Three approaches:
 - word-based tokenization :: the text is broken down into individual
   words
 - subword-based tokenization :: the text is broken down into
   collections of characters making up words
 - character-based tokenization :: the text is broken down into
   individual characters

** Fastai tokenization                                         :fastai:xxbos:
Some rules applied by fastai during tokenization are:

 - Replace repeated (non-significant) whitespace with a single space.
 - Insert a special character xxbos representing the beginning of the
   text (beginning of stream)
 - Insert special characters signifying upper case and capitalized
   words then lowercasing all words.
 - Insert special characters signfying repeated words and
   characters. This aids compression of the input saving space.

* Numericalization                                   :numericalization:vocab:

Numericalization is the process of converting the tokens into numbers
according to the vocabulary.

The vocabulary can either be infered (e.g. constructed based on the
most common tokens) or provided explicitly.

The vocabulary is represented as an ordered list of tokens and each
token in the stream is replaced by its index in this list.

** xxunk                                                              :xxunk:

Words who's frequency is below a certain threshold, for instance,
outside the most common x words or who's frequency is below the
minimum configured can be assigned the 'unknown word' token e.g. xxunk
in fastai

This reduces space required by the model and typically there's not
enough relevant data to train the model about these words.

* LMDataLoader                                                 :LMDataLoader:

Data loading process for each epoch.

1) shuffle all the entries (separate documents)
2) concatenate into single stream e.g. 50k tokens
3) Divide into mini streams according block size. e.g a block size of
   5 will give 5 mini streams of length 1000 in order: 0 - 999, 1000 -
   1999 etc
4) For each mini-stream, while retaining their order, select an offset
   of a certain sequence length and consecutively feed batches of the
   given size (block size * sequence length) in order to the model.

* Text Classification                                   :text_classification:

** padding                                        :padding:data_augmentation:

For each training epoch, the classifier needs to pass in batches of
similarly sized tensors to the learner. Since the texts to be classified
are of different lengths, padding will have to be applied to have them
all of the same size.

To do this, the text will be roughly sorted in order by length so that
for each batch, the texts are of similar sizes, which will help save
space. The texts are then padded to the length of the largest one in
the batch using a token that will be ignored by the model.

* Language model loss and metrics

Typically, the loss function used in language models is
~cross-entropy~, because predicting the next word is essentially a
classification problem of selecting the next token from the
vocabulary.

Metrics commonly used are ~perplexity~, which is the exponent of the
cross-entropy, and ~accuracy~, which is easier to interpret.

* Passing the vocabulary of the language model to classifier data block :DataBlock:vocab:

After fine tuning the pre-trained language model using corpus of the
target text to be classified, we pass the vocab of the language model
to the datablock of the classifier because we need the classifier use
the updated vocabulary for numericalization for classification rather
than use the limited one that comes with the original pre-trained
model.

* Gradual unfreezing                                     :gradual_unfreezing:

With gradual unfreezing, a pretrained model can be have target layers
fine-tuned while the rest have their weights frozen.

For example, a pre-trained language model can be frozen and trained
just to update its embeddings for the new vocabulary.

A text-classifier can be fine tuned from a language model by
unfreezing and fitting the latter task-specific layers first to
adapt them to the classification task and then eventually opening up
all layers to the fitting process.

* Machine generated text detection

Automatic detection of machine generated text is always likely to be
behind text generation techniques because automated text detection
relys on having a large corpus of machine generated text available
in order to learn them.

Therefore, new text generation techniques will always be ahead of
automated text detection techniques.

* Embedding matrix                                         :embedding_matrix:

This acts as the first layer of the neural network.

It contains a row for each index of the corpus' vocabulary.

It's shape is (vocab_size x embedding_size), where vocab_size is the
length of the vocabulary, and embedding_size is an arbitrary number
defining the number of latent factors of the tokens.
