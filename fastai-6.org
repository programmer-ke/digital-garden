#+FILETAGS: :fastai:machine_learning:tabular_data:random_forests:decision_trees
* Continuous vs categorical variables

A continuous variable has a numeric value while a categorical variable
has a finite set of possible values (a discrete set of levels).

* Two main families of machine learning algorithms

Decision trees: Good for structured data e.g. db tables
Neural networks: Good for unstructured data e.g. text, images, sound

However, structured data with a high-cardinality of levels for
categorical variables e.g. street address, movie id may benefit from
neural networks.

* Special ordering for categorical columns :decision_trees:categorical_variables:ordinals:pandas:

Some categorical variables have an implied order that a human would
recognize but not the decision tree algorithm e.g. a column that is
either "small", "medium", or "large".

We would need to let the algorithm know of this order. In pandas we
can do this by transforming the column into a category datatype with
its possible values listed in order.

* Decision tree algorithm                                    :decision_trees:

A decision tree that predicts a price, for example, can be constructed
as follows

 - For each column
   - for each level in the column
     - Calculate the average price of the two splits by that level
     - calculate the Root Mean Squared Error (rmse) of each of the two split
 - Determine the split that gives the lowest rmse values and use that
   to split the dataset into two groups
 - For each group, recursively repeat the above steps until we get to the
   termination condition (e.g. maximum number of leaf nodes in the tree, or minimum number of
   items in each group)

* Special consideration for dates in decision trees    :dates:decision_trees:

While date can be treated like a continuous variable since dates are
ordered, often we want the algorithm to be aware of some important
information in the date like day of the week, holidays etc that may 
be useful in making predictions.

We can preprocess dates by adding columns to the dataset that contain
extra information on the date like day of the week, year, month,
whether it is a special date like black friday e.t.c

* Validation set                                            :validation_sets:

When picking a validation set, we have to use one that resembles the
test set.

For example, in timeseries data, the test set will be a portion of data
from the later period in the ordered dataset because we're interested
in using the earlier data for making future predictions.

Similarly, the validation set should be split off from the later end
of the ordered training set so that we build the model with the
training set and validate by making predictions in the validation set.

* On one-hot encoding vs ordering predictor categories in decision trees: :decision_trees:categorical_variables:one_hot_encoding:fastai:

Ordering categories results in less splits for the same essentially the same outcome.

"The standard approach for nominal predictors is to consider all 2^k −
1^− 1 2-partitions of the k predictor categories. However, this
exponential relationship produces a large number of potential splits
to be evaluated, increasing computational complexity and restricting
the possible number of categories in most implementations. For binary
classification and regression, it was shown that ordering the
predictor categories in each split leads to exactly the same splits as
the standard approach. This reduces computational complexity because
only k − 1 splits have to be considered for a nominal predictor with k
categories."

https://peerj.com/articles/6339/

* bagging                                            :bagging:random_forests:

Bagging is the process of training multiple decision trees on random
subsets of the data (rows and columns) and aggregating their results
during inference.

* sklearn for random forests                           :sklearn:scikit_learn:

 - max_samples :: parameter that specifies the  number of rows
   that a tree can be trained on

 - max_features :: parameter that specifies the max number of columns
   that can be considered for splitting with each split.

 - n_estimators :: the number of random trees to train

* out of bag error                                     :out_of_bag_error:OOB:

The out of bag error is the rmse between the y-values in the training
set and the out of bag predictions for each row in the training set.

The out of bag predictions are made for each row using aggregated
predictions of the trees that didn't include it in its training
samples.

This is useful in situations were there's little data because it help
determine whether a model generalizes without needing a separate
validation set.

* model interpretation                  :random_forests:model_interpretation:

**  How confident are we in our predictions using a particular row of data?

We examine the standard deviation of the predictions on the row by
each of the trees in the random forest. The lower the standard deviation,
the higher the confidence in the predictions since the differently trained
trees gave more similar predictions.

** For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? :tree_interpreter:waterfall_graph:

We pick the row of interest and pass it through each tree in the
random forest. We take the prediction of the model at the root of the
tree as the bias for each tree. We then pass the row through the tree
and for each split, record the effect each column has on the bias,
either increasing it or reducing it. We then aggregate the deltas for
each column in the random forest and can then sort and plot it. We can
then identify the columns with the greatest effects on the row's
prediction.

To calculate this for each row, we use the tree intepreter library.
We plot the results using a waterfall plot which shows the net effect
of each column on the bias, and the net effect of all columns, which
is the final prediction.

** Which columns are the strongest predictors            :feature_importance:

We determine the strongest predictors by calculating the feature
importance.  This is done by looking at each split in each tree in the
random forest.  For each split, we not record how much the model
improves as a result, and add this to the column's importance score
(weighted by number of rows in the group).

The score for each column is aggregated, normalized and sorted,
allowing us to determine the most important features.


** How do predictions vary by the strongest predictors?

For each of the important columns, we calculate the partial dependence.
This is done by replacing the column's value in all the rows with a
specific value and making predictions. For each possible value of this
column in the dataset, we make predictions across all rows and average the
final prediction.

We can then plot a partial dependence graph for each column mapping each
value with the resulting average prediction across all rows.

By this, we can tell how the column's values affect the prediction, all
other things being equal.

* Removing unimportant features                          :feature_importance:

Removing unimportant features helps minimize the model complexity
and increase it's understandability without significantly impacting
the model's accuracy.

It results in fewer splits in the decision tree hence reducing training
and inference time and helps focusing the analysis on only the important
factors.

This can be done by using the feature importance to remove the columns
with low importance from the dataset and removing all but one of
similar columns that are highly correlated with each other.


    If you increase n_estimators to a very high value, can that lead
    to overfitting? Why or why not?

    Make a list of reasons why a model's validation set error might be
    worse than the OOB error. How could you test your hypotheses?


    What is the "extrapolation problem"?
    How can you tell if your test or validation set is distributed in a different way than your training set?
    Why do we ensure saleElapsed is a continuous variable, even although it has less than 9,000 distinct values?
    What is "boosting"?
    How could we use embeddings with a random forest? Would we expect this to help?
    Why might we not always use a neural net for tabular modeling?

    Provide two of the words that are used for the possible values of
    a categorical variable.
    What is a "dense layer"?
    How do entity embeddings reduce memory usage and speed up neural networks?
    What kinds of datasets are entity embeddings especially useful for?
