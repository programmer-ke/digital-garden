#+FILETAGS: :fastai:machine_learning:tabular_data:random_forests:decision_trees
* Continuous vs categorical variables

A continuous variable has a numeric value while a categorical variable
has a finite set of possible values (a discrete set of levels).

* Two main families of machine learning algorithms

Decision trees: Good for structured data e.g. db tables
Neural networks: Good for unstructured data e.g. text, images, sound

However, structured data with a high-cardinality of levels for
categorical variables e.g. street address, movie id may benefit from
neural networks.

* Special ordering for categorical columns :decision_trees:categorical_variables:ordinals:pandas:

Some categorical variables have an implied order that a human would
recognize but not the decision tree algorithm e.g. a column that is
either "small", "medium", or "large".

We would need to let the algorithm know of this order. In pandas we
can do this by transforming the column into a category datatype with
its possible values listed in order.

* Decision tree algorithm                                    :decision_trees:

A decision tree that predicts a price, for example, can be constructed
as follows

 - For each column
   - for each level in the column
     - Calculate the average price of the two splits by that level
     - calculate the Root Mean Squared Error (rmse) of each of the two split
 - Determine the split that gives the lowest rmse values and use that
   to split the dataset into two groups
 - For each group, recursively repeat the above steps until we get to the
   termination condition (e.g. maximum number of leaf nodes in the tree, or minimum number of
   items in each group)

* Special consideration for dates in decision trees    :dates:decision_trees:

While date can be treated like a continuous variable since dates are
ordered, often we want the algorithm to be aware of some important
information in the date like day of the week, holidays etc that may 
be useful in making predictions.

We can preprocess dates by adding columns to the dataset that contain
extra information on the date like day of the week, year, month,
whether it is a special date like black friday e.t.c

* Validation set                                            :validation_sets:

When picking a validation set, we have to use one that resembles the
test set.

For example, in timeseries data, the test set will be a portion of data
from the later period in the ordered dataset because we're interested
in using the earlier data for making future predictions.

Similarly, the validation set should be split off from the later end
of the ordered training set so that we build the model with the
training set and validate by making predictions in the validation set.

* On one-hot encoding vs ordering predictor categories in decision trees: :decision_trees:categorical_variables:one_hot_encoding:fastai:

Ordering categories results in less splits for the same essentially the same outcome.

"The standard approach for nominal predictors is to consider all 2^k −
1^− 1 2-partitions of the k predictor categories. However, this
exponential relationship produces a large number of potential splits
to be evaluated, increasing computational complexity and restricting
the possible number of categories in most implementations. For binary
classification and regression, it was shown that ordering the
predictor categories in each split leads to exactly the same splits as
the standard approach. This reduces computational complexity because
only k − 1 splits have to be considered for a nominal predictor with k
categories."

https://peerj.com/articles/6339/

* bagging                                            :bagging:random_forests:

Bagging is the process of training multiple decision trees on random
subsets of the data (rows and columns) and aggregating their results
during inference.

* sklearn for random forests                           :sklearn:scikit_learn:

 - max_samples :: parameter that specifies the  number of rows
   that a tree can be trained on

 - max_features :: parameter that specifies the max number of columns
   that can be considered for splitting with each split.

 - n_estimators :: the number of random trees to train

* out of bag error                                     :out_of_bag_error:OOB:

The out of bag error is the rmse between the y-values in the training
set and the out of bag predictions for each row in the training set.

The out of bag predictions are made for each row using aggregated
predictions of the trees that didn't include it in its training
samples.

This is useful in situations were there's little data because it help
determine whether a model generalizes without needing a separate
validation set.

* model interpretation                  :random_forests:model_interpretation:

**  How confident are we in our predictions using a particular row of data?

We examine the standard deviation of the predictions on the row by
each of the trees in the random forest. The lower the standard deviation,
the higher the confidence in the predictions since the differently trained
trees gave more similar predictions.

** For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? :tree_interpreter:waterfall_graph:

We pick the row of interest and pass it through each tree in the
random forest. We take the prediction of the model at the root of the
tree as the bias for each tree. We then pass the row through the tree
and for each split, record the effect each column has on the bias,
either increasing it or reducing it. We then aggregate the deltas for
each column in the random forest and can then sort and plot it. We can
then identify the columns with the greatest effects on the row's
prediction.

To calculate this for each row, we use the tree intepreter library.
We plot the results using a waterfall plot which shows the net effect
of each column on the bias, and the net effect of all columns, which
is the final prediction.

** Which columns are the strongest predictors            :feature_importance:

We determine the strongest predictors by calculating the feature
importance.  This is done by inspecting each split in each tree in the
random forest.  For each split, we record how much the model
improves as a result, and add this to the column's importance score
(weighted by number of rows in the group).

The score for each column is aggregated, normalized and sorted,
allowing us to determine the most important features.


** How do predictions vary by the strongest predictors?

For each of the important columns, we calculate the partial dependence.
This is done by replacing the column's value in all the rows with a
specific value and making predictions. For each possible value of this
column in the dataset, we make predictions across all rows and average the
final prediction.

We can then plot a partial dependence graph for each column mapping each
value with the resulting average prediction across all rows.

By this, we can tell how the column's values affect the prediction, all
other things being equal.

* Removing unimportant features                          :feature_importance:

Removing unimportant features helps minimize the model complexity
and increase it's understandability without significantly impacting
the model's accuracy.

It results in fewer splits in the decision tree hence reducing training
and inference time and helps focusing the analysis on only the important
factors.

This can be done by using the feature importance to remove the columns
with low importance from the dataset and removing all but one of
similar columns that are highly correlated with each other.

* The extrapolation problem :extrapolation_problem:random_forests:decision_trees:

The extrapolation problem happens when a model is unable to predict
values outside the range it encountered in training.

An example of this is when decision trees are used to predict trend
over time, which gives values that are lower than actual figures. This
is because a decision tree estimates the average value of each group,
for data points that lie beyond the timeline used for training, the
predicted value will be close to that of the tail end of the training
set.

* Finding out of domain data                :domain_shift:out_of_domain_data:

To tell if the validation set is different distributed differently
from the training data, the model can be trained on the combined set
of training and validation data to predict whether a row is found
in the training or validation set.

After training the model, the feature importances of the columns
can be examined to determine which columns are highly correlated 
with the train/validate split.

Columns that do not significantly lower the prediction accuracy
can them be excluded.

* Dates as continuous variables                  :continuous_variables:dates:

We make sure that in tabular data, dates are treated as
continuous variables so that predictions can be made for values outside
the training domain.

* boosting                                                :boosting:xg_boost:

Boosting methods are where predictions by an ensemble of models are
added to each other to predict the final value.

The process is as follows using trees:

 1) A tree that underfits is trained on input data.
 2)For each data point, the difference between it's prediction and 
  target values is calculated.
 3) This differences become the new target values
 4) The process repeats from step 1, until we reach some stopping
    criteria e.g. max no of trees or overfitting

During inference, the predictions of each tree will be added up
to get the final predictions.
