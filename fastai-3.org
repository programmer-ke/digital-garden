#+FILETAGS: :fastai:machine_learning:deep_learning

* MNIST_SAMPLE file structure                                         :mnist:

There are 2 directories, =train= and =valid=. Once can train using images in the
former directory and validate using the latter.

Inside each of =train= and =valid=, we have directories that serve
as labels for the data e.g. images of number 7 will be under the 
directory =7=.

*  Pixel similarity approach to image classification :pixel_similarity:mae:mad:rmse:l1_norm:l2_norm:

Take all the images of a particular object e.g. number 7 and find the
average of each pixel position. This will give an /ideal/ image of that
object that is an average of all samples.

To classify a new image, calculate the distance between the pixels of
that image and the ideal image to find out how far apart they are.

Two ways of doing that are:
 - Mean Absolute Difference (MAD)/ L1 Norm :: get the absolute difference for each
   pixel position then calculate the mean
 - Root Mean Squared Error (RMSE) / L2 Norm :: get the square of the differences
   for each pixel position, the the mean of the square values then
   finally get the square root.

* Tensor rank vs shape                                    :rank:shape:tensor:

- rank :: The number of axes/dimensions of the tensor.
- shape :: A list showing the length of each axis/dimension.

A rank-3 tensor is a tensor with 3 axis i.e. a cube.

* Parallel computation of thousands of numbers with python :numpy:pytorch:tensor:array:gpu:cuda:

Rather than use python itself, we can use a lower language for better
performance. Two common ways are using =numpy=, which will implement
the operations in optimized C, and =pytorch=, which will implement
the operations parallelized on a GPU via CUDA if available.

=numpy= provides the ~array~ data structure for this, while =pytorch=
provides the ~tensor~ data structure.

On a GPU, the operation will be many thousands of times faster than
pure python.

* Broadcasting                                  :tensor:pytorch:broadcasting:

When conducting an operation between two pytorch tensors, if one of
the tensors is of a lower rank, it will be broadcast onto the one with
the larger rank.

e.g. on adding a tensor of shape (3, 3, 3) with another of shape (3, 3),
the latter will be broadcast across one axis of the former one 3 times.

* Image Representation in computers    :image_representation:grayscale:color:

An image is typically represented using numbers in a computer.

For example, a gray scale image of 28 * 28 pixels will be represented
as a matrix of shape (28, 28) with each cell having a number between
0 and 255 representing the shade of grey.

A colour image will have red, green and blue values for each
pixel. Therefore, a 28 * 28 colour image will be represented by a
cube of shape (28, 28, 3), with each cell having a number between 0
and 255 representing the shade of the colour red, green or blue.

* Metrics                                            :metrics:validation_set:

Metrics are generally calculated by the validation set. This is because
we want to know how the model is performing on data it hasn't yet seen.

* Stochastic Gradient Descent                                 :SGD:gradients:

SGD is an iterative method of updating the weights of a neural network
to improve their predictions. This is done by defining function that
calculates the loss, i.e. how far off the predictions are from the
actual outcome, and updating the weights of the network to minimize
this loss.

The process of updating the weights is called backward propagation.
For each weight, calculus is used to calculate the value of the
derivative of the loss function with respect to it. The collection
of these values for each of the weights (parameters) are called the
gradients and represent the slope of the function for each parameter.

The goal is for these gradients to be as close to zero as possible,
so we update the weights in the direction that makes this true.

Typical loss functions are mean absolute error (MAE) and mean squared
error (MSE).

The seven steps in SGD are:

 1) Initialize parameters (random)
 2) Calculate predictions
 3) Calculate loss
 4) Calculate gradients that minise the loss
 5) Update the weights based on the gradients
 6) While not yet good enough, jump to 2)
 7) Stop

* Learning rate                                :hyperparameter:learning_rate:

The learning rate determines by what extent we update the model
parameters often expressed as a percentage of the gradients per
parameter.

A large percentage implies a greater jump in the direction that
optimizes the gradient.

While it may converge to the optimal value faster there's a risk of
overshooting the optimal value in the graph of the function we're
optimizing meaning a similarly large adjustment will have to be made
to the parameters in the opposite direction. This may make the
function approach the optimal value at a slower rate as it oscillates
around the optimal value.

* Mini Batches in SGD                                      :sgd:mini_batches:

While updating parameters of a neural network, we typically need to
learn from a collection of data items in order to learn general
patterns, rather than learning from single items in sequence which
exhibits a lot of variability between them.

However, updating the parameters after a whole pass through the entire
dataset would result if very slow learning cycles.

A mini-batch helps us compromise between learning from a high
variability between individual items in the dataset, and the
generality of the entire data set.

For each pass through the data set, the dataloader will shuffle and
batch items so to prevent the model from learning from the order of
items the training set.

* Accuracy Metric vs Loss :accuracy:loss:mae:mse:gradients:calculus:classification:

The accuracy metric is useful for humans evaluating the performance of
a model because it tells us the percentage of items that we classified
correctly by the model.

However, in attempting to use the accuracy metric as a loss function
in SGD, we run into an issue where small changes in parameters may
result in no changes to the accuracy of the model.

Because we use calculus in calculating the gradients used in SGD,
using accuracy as a metric will result in large parts of the function
of the graph where the output is constant (0.5 threshold not crossed
for altering classification) and therefore gradients being zero.

At the point where the threshold that alters is crossed, the gradient
will be infinity. 

We therefore need to use a function that shows minute changes in the
output for minute changes to the input parameters. That's why we
typically use something like MAE or MSE.

* tensor().view                                                :pytorch:view:

The view method of a pytorch tensor returns another tensor with the
same number of items as the original but with a different shape.

If any axis has ~-1~, it will be made as large as necessary to fit
all the data.

* bias parameters                                                      :bias:

Bias parameters are used to make more general functions that can
have a varied y-intercept. Without the bias, all the functions
expressed by a neural network layer have the y-intercept at zero.

* backward pass, forward pass                      :pytorch:backward:forward:

In ml terms, a forward pass implies making a prediction and a backward
pass implies calculating gradients from the outcome and loss.

The backward method on a tensor of the loss output tensor calculates
the gradients for each of the parameters used in the forward pass.

We have to zero the gradients before the next call to backwards because
pytorch will by default add new gradients to the existing ones.

* Rectified Linear unit :ReLU:relu:activation_function:universal_approximation_theorem:

A rectified linear unit is a function that outputs a minimum of
of zero for all input ranges.

An activation function is a layer in the neural network consisting of
a ReLU.

It is used to separate 2 linear functions in a neural network. This is
because we need linear functions in different layers of a neural
network behave differently, yet the composition of one or more linear
functions results in another linear function. The activation function
(or nonlinearity) prevents this coupling of linear function layers.

The universal approximation theorem shows that any function can be
approximated as closely as possible with one nonlinearity in between
two linear units. However, in practice, we use more layers because it
is easier to get better models with less parameters using a more narrow
layers, than use just two linear units with a large number of parameters.
