#+FILETAGS: :fastai:

* Areas where deep learning is now the best in the world :deep_learning:chatgpt:
 - Computer Vision :: self-driving cars, face recognition
 - NLP :: surmarizing documents, chatbots e.g. openGPT
 - Medicine :: finding anomalies in radiology images
 - Image Generation :: Colourizing images, stable diffusion
 - Games :: Chess, Go
 - Biology :: Protein folding
 - Recommendation systems :: movies, shopping, search

* Artificial Neural Networks         :ann:artificial_neural_networks:history:
The first device based on the principle of the artificial neuron was
the Mark 1 Perceptron. It was designed for image recognition, with
20 * 20 photocells connected to weights.

https://en.wikipedia.org/wiki/Perceptron

* Parallel Distributed Processing requirements                          :pdp:
Based on the 1986 book MIT press, requirements for a parallel
distributed system were:

 - A set of processing units
 - Interconnection between the processing units
 - A state of activation
 - An output function for each unit
 - A propagation rule for propagating patterns of activities through
   the network
 - An activation rule combining inputs to a unit with the current
   state of the unit to give an output from the unit
 - A learning rule where patterns of connectivity are modified by
   experience
 - An environment within which the system must operate

* Misunderstandings that held back the field of neural networks
 - That they were incapable of learning some simple mathematical
   functions like XOR

* GPU                                                                   :gpu:
A graphics processing unit is a specialized computer processor that executes
a computer instruction in parallel on many data points.

* Image recognition using traditional computer programs   :image_recognition:

It is hard to recognize images using traditional computer programs
because traditional programming works by specifying a step by step
procedure to achieve an outcome in detail.

It is extremely hard to write a step by step procedure to instruct a
computer to recognize a single image, let along generalize it.

* Machine learning description by Arthur Samuel (1949) :machine_learning:arthur_samuel:ml:

By weight assignment, Samuel meant values assigned to variables which
affected how his program made decisions.

In deep learning, we use parameters in place of what Samuel called
weights. A weight is a specific type of parameter in deep learning.

** Samuel's view of the machine learning model

| Source      | Action         | Destination/Output |
|-------------+----------------+--------------------|
| Inputs      | provided to    | Model              |
| Weights     | provided to    | Model              |
| Model       | produces       | Results            |
| Results     | evaluated into | Performance        |
| Performance | updates        | Weights            |

Input ------> Model --------> Results ----> Performance 
               ^                               |
               |                               |
Weights -------                                |
   ^                                           |
   |                                           v
    <-------------------------------------------

In his view the process of updating weights to optimize performance
could be made automatic.

* Deep learning explainability

It is hard to understand why a deep learning model makes a particular
prediction because prediction is emergent behaviour from many discrete
units interconnected across many layers.

It is less deterministic than traditional programming.

* The Universal Approximation Theorem :theorem:proof:universal_approximation_theorem:neural_network:

This is a proof that shows that a mathematical function modeling a
neural network can solve any problem to any level of accuracy

* Requirements needed to train a model

 - Labelled datasets

* Feedback loops when making prediction                      :feedback_loops:

A positive feedback loop can develop where a model's predictions
enhance bias in the data used to train it e.g. a predictive policing
model trained on datasets of areas where arrests have been made in the
past will lead to the police make more arrests in those areas when
deployed to predict crimes, which further produces data supporting its
bias.

* Image size for image recognition                    :image_recognition:224:

Historically 224*224 was used to train image models, so pre-trained
models would require this size. However, higher-resolution images can
be used which will lead to improved accuracy at the expense of higher
computational costs.

* Classification vs Regression                    :classification:regression:

 - Classification :: Assigning a class to the input
 - Regression :: predict a numerical value

* Types of datasets                            :datasets:train:test:validate:

 - Training set :: Used to update model parameters
 - Validation set :: Used to evaluate the generality of the model
   weights by testing against data not seen by model
 - Test set :: Used to evaluate the generality of the model's
   hyperparameters not seen by model developers, since they have
   already seen the validation set

* Sampling for validation sets                 :validate:validation:datasets:

Many times a random sample of the provided sample will be used as a
validation set, but not always. With timeseries data, the approach
used is to train on a continuous batch of earlier data and validate
using a continuous batch of later data.

This more closely matches what the model should do in production
i.e. make future predictions based on past data.

* overfitting                                          :overfitting:training:

This is a situation where a model learns specific characteristings of 
the training dataset rather than patterns that can be generalized for
data not yet seen.

This is noticeable when a model's accuracy is increasing on the
training data, but decreasing on the validation data.

This happens when you train to long on insufficient data.

* metric vs loss                                            :metric:loss:sgd:

 - metric :: Tells the model developer how well the model
   performs. Optimized for developer
 - loss :: Informs the SGD algorithm how well the model performs and
   is used primarily for updating the model weights. Optimized for SGD

* pretrained models                                       :transfer_learning:

Pretrained models help in minimizing training time and data
needed. They are already trained by experts on big data and so already
contain generalized layers that are useful to the task at hand.

** Head of a model                                                     :head:

The head of a model is the last layer. It is usually the most specific
to the training data used.

When fine tuning with the fast ai library, the head is typically the
layer that is customized for the dataset provided.

The earlier layers contain more generalized and primitive features
e.g. edges or color for vision models.

The later layers contain higher level features that are usually more
specialized to the training dataset e.g. car wheels in a dataset of
cars.

* image recognizers on non-image tasks

Image recognition models can be used to learn non-image tasks by
having them converted to an image representation e.g. having the
binary-encoding of malware being represented as an image.

* model architecture                                           :architecture:

A model architecture is the arrangment of layers and their
interconnections.  It is a template for the mathematical function provided
by parameters.

* segmentation           :segmentation:self_driving_cars:autonomous_vehicles:

Segmentation is the process of using a model to recognize the content
of each pixel in an image.

* fastai y_range                                                    :y_range:

y_range is useful when we're predicting a continuous value rather
than producing a classification e.g. predicting movie rating based on review

* hyperparameters                                           :hyperparameters:

Hyperparameters are model parameters that configure how it trains, and
are typically set by the model developer rather than by the training
data like model weights.

* validation                   :validation:failures:datasets:hyperparameters:

The best way to avoid failures when using AI in an organization is to
have a reserved test set representative of actual expected data that
is not made available during the model training phase. This can detect
cases where we overfit the hyperparameters to the validation set.
