#+FILETAGS: :fastai:machine_learning:deep_learning

* Text models deficiencies                                          :llm:nlp:

Models can generate text that is very compelling yet incorrect. This
is a risk where correctness is very important e.g. precedents in
judgement used in court of law.

A layman may be deceived by very compelling generated information that
only an expert would identify as incorrect.

This has implications on spreading disinformation at a large scale
online, for instance.

* ML Automation                                                  :automation:

In situations where a model might make mistakes, a good alternative to
automating a process is to insert a human into the loop to verify the
model predictions.

Automated alerts from predictions can help timely intervention in
time-sensitive situations, but channelling the alerts through a human
ensures reduces the error rate.

* Tabular Data and Deep learning                       :tabular_data:xgboost:

Generally, tabular data work well with model ensembles like
Xgboost. However, deep learning is suited in cases where certain
columns have a high cardinality (large number of options), or include
info amenable to deep learning such as natural language or images.

The downside is that deep learning models typically take longer to
train than the other methods used for tabular data.

* Deep learning in recommendation systems            :recommendation_systems:

While deep learning may be useful in recommendation systems where
information is represented as a sparse matrix (users as rows, products
as columns), and make recommendations by attempting the fill up the matrix,
they are only good at predicting what a user might like, but not what may
be useful to them.

For example, they may recommend variations of a product already
consoomed, but not an entirely new one that may be useful.

* Steps of the Drivetrain approach

 1) Identify the objective
 2) Identify levers (controllable actions) that can be used to achieve the objective
 3) Collect data needed (existing or new)
 4) Construct the models that can make predictions that lead to the appropriate action

In self driving:

 1) Get from A to B without engaging the car
 2) Intercepting the car controls - steering, braking, acceleration...
 3) Collect driving footage, record front-view, rear view, proximity
    sensors etc..
 4) A single model, or model assembly with a simulator (search space
    of possible outcomes) and optimizer (selects best possible
    outcome)

    Question: How does end-to-end learning come into the picture here?
    Possible Answer: End to end learning is an attempt at avoiding
    hand-engineered features in deep learning (information is lost) if
    data is plentiful. Possible approach in here is a single deep
    learning model that is trained straight from all inputs.
 


    Where do text models currently have a major deficiency?
    What are possible negative societal implications of text generation models?
    In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process?
    What kind of tabular data is deep learning particularly good at?
    What's a key downside of directly using a deep learning model for recommendation systems?
    What are the steps of the Drivetrain Approach?
    How do the steps of the Drivetrain Approach map to a recommendation system?
    

    Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data.
    Create an image recognition model using data you curate, and deploy it on the web.
    What is DataLoaders?
    What four things do we need to tell fastai to create DataLoaders?
    What does the splitter parameter to DataBlock do?
    How do we ensure a random split always gives the same validation set?
    What letters are often used to signify the independent and dependent variables?
    What's the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?
    What is data augmentation? Why is it needed?
    What is the difference between item_tfms and batch_tfms?
    What is a confusion matrix?
    What does export save?
    What is it called when we use a model for getting predictions, instead of training?
    What are IPython widgets?
    When might you want to use CPU for deployment? When might GPU be better?
    What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?
    What are three examples of problems that could occur when rolling out a bear warning system in practice?
    What is "out-of-domain data"?
    What is "domain shift"?
    What are the three steps in the deployment process?

